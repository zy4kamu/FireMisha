{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "sentiment-fine-tuning-huggingface.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zy4kamu/FireMisha/blob/main/intent_classifier_and_slot_filler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z83hr7MWesMW"
      },
      "source": [
        "# Step 0: preliminary actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "633fetsKg5cv",
        "outputId": "492f6822-eaa9-4070-818c-ad94000baf60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "# Install HuggingFace transformers\n",
        "\n",
        "!pip install git+https://github.com/huggingface/transformers.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-3x3kucvq\n",
            "  Running command git clone -q https://github.com/huggingface/transformers.git /tmp/pip-req-build-3x3kucvq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied (use --upgrade to upgrade): transformers==3.3.1 from git+https://github.com/huggingface/transformers.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (2.23.0)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (0.8.1rc2)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (1.18.5)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (0.1.91)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (4.41.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.3.1) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.3.1) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.3.1) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3.1) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3.1) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.3.1) (2.4.7)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-3.3.1-cp36-none-any.whl size=1082350 sha256=75bb1cd6dc87be6376acbbcac121b538bc594b91ac26c76c4dc741ffb806739c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-blocrj0x/wheels/33/eb/3b/4bf5dd835e865e472d4fc0754f35ac0edb08fe852e8f21655f\n",
            "Successfully built transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXCmZaspWTCT",
        "outputId": "51694148-39a2-48ae-a0a5-4b528efad106",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# Download dataset for training\n",
        "\n",
        "!rm -rf nlu-benchmark\n",
        "!git clone https://github.com/sonos/nlu-benchmark.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'nlu-benchmark'...\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 400 (delta 2), reused 0 (delta 0), pack-reused 389\u001b[K\n",
            "Receiving objects: 100% (400/400), 1.19 MiB | 13.88 MiB/s, done.\n",
            "Resolving deltas: 100% (248/248), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pi31_2cndZU"
      },
      "source": [
        "# Import required packages and set logging verbosity\n",
        "\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import transformers\n",
        "from transformers import DistilBertTokenizerFast\n",
        "from transformers import TFDistilBertForSequenceClassification\n",
        "from transformers import TFDistilBertForTokenClassification\n",
        "\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "transformers.logging.set_verbosity_error()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60Jv3FVuud4b"
      },
      "source": [
        "# Step 1: Train intent classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucAksNc8e1N2"
      },
      "source": [
        "# Create training/validation dataset and save files to disk\n",
        "\n",
        "def parse_files(input_files, output_file, labels):\n",
        "    def parse_file(input_file, labels, output):\n",
        "        with open(input_file, errors='ignore') as reader:\n",
        "            data = json.load(reader)\n",
        "        assert len(data) == 1\n",
        "        label = list(data.items())[0][0]\n",
        "        if not label in labels:\n",
        "          labels.append(label)\n",
        "        label_index = labels.index(label)\n",
        "        data = list(data.items())[0][1]\n",
        "        for sentence in data:\n",
        "            sentence = sentence['data']\n",
        "            text = ''.join([_['text'] for _ in sentence]).lower()\n",
        "            output.append(json.dumps({'text':text, 'label':label_index}))\n",
        "        return output\n",
        "\n",
        "    output = []\n",
        "    for input_file in input_files:\n",
        "        parse_file(input_file, labels, output)\n",
        "    random.shuffle(output)\n",
        "    with open(output_file, 'w') as writer:\n",
        "        writer.write('\\n'.join(output))\n",
        "    with open('intent_classes.json', 'w') as writer:\n",
        "        writer.write(json.dumps(labels))\n",
        "\n",
        "\n",
        "input_files = [\n",
        "    'nlu-benchmark/2017-06-custom-intent-engines/SearchCreativeWork/train_SearchCreativeWork_full.json',\n",
        "    'nlu-benchmark/2017-06-custom-intent-engines/RateBook/train_RateBook_full.json',\n",
        "    'nlu-benchmark/2017-06-custom-intent-engines/SearchScreeningEvent/train_SearchScreeningEvent_full.json',\n",
        "    'nlu-benchmark/2017-06-custom-intent-engines/AddToPlaylist/train_AddToPlaylist_full.json',\n",
        "    'nlu-benchmark/2017-06-custom-intent-engines/PlayMusic/train_PlayMusic_full.json',\n",
        "    'nlu-benchmark/2017-06-custom-intent-engines/GetWeather/train_GetWeather_full.json',\n",
        "    'nlu-benchmark/2017-06-custom-intent-engines/BookRestaurant/train_BookRestaurant_full.json'\n",
        "]\n",
        "intent_classes = []\n",
        "parse_files(input_files, 'train_classification_dataset.json', intent_classes)\n",
        "\n",
        "input_files = [\n",
        "    'nlu-benchmark/2017-06-custom-intent-engines/SearchCreativeWork/validate_SearchCreativeWork.json',\n",
        "    'nlu-benchmark/2017-06-custom-intent-engines/RateBook/validate_RateBook.json',\n",
        "    'nlu-benchmark/2017-06-custom-intent-engines/SearchScreeningEvent/validate_SearchScreeningEvent.json',\n",
        "    'nlu-benchmark/2017-06-custom-intent-engines/AddToPlaylist/validate_AddToPlaylist.json',\n",
        "    'nlu-benchmark/2017-06-custom-intent-engines/PlayMusic/validate_PlayMusic.json',\n",
        "    'nlu-benchmark/2017-06-custom-intent-engines/GetWeather/validate_GetWeather.json',\n",
        "    'nlu-benchmark/2017-06-custom-intent-engines/BookRestaurant/validate_BookRestaurant.json'\n",
        "]\n",
        "parse_files(input_files, 'validate_classification_dataset.json', intent_classes)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "UZsRQ-N8ndZa",
        "outputId": "cdf22b6c-e7b3-4973-e959-742aa7c4b229",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "# Read training and validate datasets\n",
        "\n",
        "def read_dataset(input_file):\n",
        "  sentences = []\n",
        "  labels = []\n",
        "  with open(input_file) as reader:\n",
        "    for line in reader:\n",
        "      item = json.loads(line)\n",
        "      sentences.append(item['text'])\n",
        "      labels.append(item['label'])\n",
        "  return sentences, labels\n",
        "\n",
        "def read_intent_classes(input_file):\n",
        "  with open(input_file) as reader:\n",
        "    return json.load(reader)\n",
        "\n",
        "training_sentences, training_labels = \\\n",
        "  read_dataset('train_classification_dataset.json')\n",
        "validation_sentences, validation_labels = \\\n",
        "  read_dataset('validate_classification_dataset.json')\n",
        "intent_classes = read_intent_classes('intent_classes.json')\n",
        "\n",
        "print('Intent classes:', intent_classes)\n",
        "print('Training sentences:', training_sentences[0:5], '...')\n",
        "print('Training labels:', training_labels[0:5], '...')\n",
        "print('Validation sentences:', validation_sentences[0:5], '...')\n",
        "print('Validation labels:', validation_labels[0:5], '...')\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Intent classes: ['SearchCreativeWork', 'RateBook', 'SearchScreeningEvent', 'AddToPlaylist', 'PlayMusic', 'GetWeather', 'BookRestaurant']\n",
            "Training sentences: ['i want to see the tv series a state of mind', 'add the song by brian larsen to the cardio playlist', 'book a reservation for 7 people at the french laundry on june the 8th, 2029', 'book for jessie, dale wright and lupe at a bistro on feb. 20, 2040', 'play some rock & roll by deezer.'] ...\n",
            "Training labels: [0, 3, 6, 6, 4] ...\n",
            "Validation sentences: ['is we are northern lights playing in any movie theatre', 'is patrick still lives showing at amc theaters', 'can i see ellis island revisited in 1 minute', 'i want to book a restaurant in niger for seven people.', 'please look up the novel, live to dance.'] ...\n",
            "Validation labels: [2, 2, 2, 6, 0] ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "KCxtcxObndZk"
      },
      "source": [
        "# Create training and validation datasets for TensorFlow backend\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "train_encodings = tokenizer(training_sentences,\n",
        "                            truncation=True,\n",
        "                            padding=True)\n",
        "val_encodings = tokenizer(validation_sentences,\n",
        "                          truncation=True,\n",
        "                          padding=True)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    training_labels\n",
        "))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(val_encodings),\n",
        "    validation_labels\n",
        "))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "8_gjepLSndZq",
        "outputId": "03079118-27af-4335-d629-0c24ccf57a02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# Train the model\n",
        "\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',\n",
        "                                                              num_labels=len(intent_classes))\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy'])\n",
        "model.fit(train_dataset.shuffle(100).batch(16),\n",
        "          epochs=3,\n",
        "          batch_size=16,\n",
        "          validation_data=val_dataset.shuffle(100).batch(16))\n",
        "model.save_pretrained('classification_model')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "862/862 [==============================] - 74s 85ms/step - loss: 0.1445 - accuracy: 0.9657 - val_loss: 0.0466 - val_accuracy: 0.9843\n",
            "Epoch 2/3\n",
            "862/862 [==============================] - 75s 86ms/step - loss: 0.0407 - accuracy: 0.9894 - val_loss: 0.0549 - val_accuracy: 0.9829\n",
            "Epoch 3/3\n",
            "862/862 [==============================] - 76s 88ms/step - loss: 0.0277 - accuracy: 0.9917 - val_loss: 0.0523 - val_accuracy: 0.9886\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "akGTf-l_ndZy",
        "outputId": "78b25320-c9a1-4b85-9a00-6817131500db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "# Load model and make prediction on specific sentence \n",
        "\n",
        "classification_model = TFDistilBertForSequenceClassification.from_pretrained('classification_model')\n",
        "\n",
        "\n",
        "def get_intent_class(test_sentence):\n",
        "  predict_input = tokenizer.encode(test_sentence,\n",
        "                                  truncation=True,\n",
        "                                  padding=True,\n",
        "                                  return_tensors='tf')\n",
        "  probabilities = tf.nn.softmax(classification_model.predict(predict_input)[0]).numpy()[0]\n",
        "  max_index = np.argmax(probabilities)\n",
        "  return intent_classes[max_index], probabilities\n",
        "\n",
        "\n",
        "print()\n",
        "intent_class, probabilities = get_intent_class('play a jack lawrence concerto')\n",
        "for label, probability in zip(intent_classes, probabilities):\n",
        "  print('{}: {:0.5f}'.format(label, probability))\n",
        "print()\n",
        "print('Best intent class:', intent_class)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "SearchCreativeWork: 0.00201\n",
            "RateBook: 0.00006\n",
            "SearchScreeningEvent: 0.00009\n",
            "AddToPlaylist: 0.00026\n",
            "PlayMusic: 0.99749\n",
            "GetWeather: 0.00003\n",
            "BookRestaurant: 0.00006\n",
            "\n",
            "Best intent class: PlayMusic\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xcw4p0EHupA5"
      },
      "source": [
        "# Step 2: train slot filler for AddToPlayList dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CahFOikiWqsz"
      },
      "source": [
        "# Create training dataset and save files to disk\n",
        "\n",
        "\n",
        "def parse_files(input_files, output_file, \n",
        "                classification_restrict_dic, entity_dic):\n",
        "    def parse_file(input_file, classification_restrict_dic, entity_dic, output):\n",
        "        with open(input_file, errors='ignore') as reader:\n",
        "            data = json.load(reader)\n",
        "        assert len(data) == 1\n",
        "        classification_label = list(data.items())[0][0]\n",
        "        data = list(data.items())[0][1]\n",
        "        if classification_label not in classification_restrict_dic:\n",
        "          classification_restrict_dic[classification_label] = []\n",
        "        for sentence in data:\n",
        "            sentence = sentence['data']\n",
        "            text = ''.join([_['text'] for _ in sentence]).lower()\n",
        "            tokens = tokenizer.tokenize(text)\n",
        "            token_indexes = tokenizer(text)\n",
        "            char_indexes = get_char_indexes_from_tokenized(tokens)\n",
        "            segment_indexes = get_segment_indexes(sentence)\n",
        "            entities = [get_entity(_, segment_indexes, entity_dic) for _ in \n",
        "                        char_indexes]\n",
        "            for entity in entities:\n",
        "              if not entity in classification_restrict_dic[classification_label]:\n",
        "                classification_restrict_dic[classification_label].append(entity)\n",
        "            to_add = {'input_ids':cut(token_indexes['input_ids']), \n",
        "                      'attention_mask':cut(token_indexes['attention_mask']), \n",
        "                      'labels':cut(entities),\n",
        "                      'tokens':cut(tokens)}\n",
        "            output.append(json.dumps(to_add))\n",
        "        return output\n",
        "\n",
        "    def cut(data):\n",
        "        MAX_SEQ_LENGTH = 41\n",
        "        if len(data) > MAX_SEQ_LENGTH:\n",
        "            data = data[:MAX_SEQ_LENGTH]\n",
        "        while len(data) < MAX_SEQ_LENGTH:\n",
        "            data.append(0)\n",
        "        return data\n",
        "\n",
        "    def get_char_indexes_from_tokenized(tokens):\n",
        "        indexes = []\n",
        "        index = -1\n",
        "        for token in tokens:\n",
        "            if not token.startswith('##'):\n",
        "                index += 1\n",
        "            indexes.append(index)\n",
        "            index += len(token.replace('##', ''))\n",
        "        return indexes\n",
        "\n",
        "    def get_segment_indexes(sentence):\n",
        "        segments = []\n",
        "        index = 0\n",
        "        for item in sentence:\n",
        "            if 'entity' in item:\n",
        "                segments.append((item['entity'], index, index + len(item['text'])))\n",
        "            index += len(item['text'])\n",
        "        return segments\n",
        "\n",
        "    def get_entity(index, sentence, entity_dic):\n",
        "        val = ''\n",
        "        for entity, start, end in sentence:\n",
        "            if index >= start and index < end:\n",
        "                val = entity\n",
        "                break\n",
        "        if val not in entity_dic:\n",
        "            ind = len(entity_dic)\n",
        "            entity_dic[val] = ind\n",
        "        return entity_dic[val]\n",
        "\n",
        "    output = []\n",
        "    for input_file in input_files:\n",
        "        parse_file(input_file, classification_restrict_dic, entity_dic, output)\n",
        "\n",
        "    entity_dic = sorted(entity_dic.items(), key=lambda x: x[1])\n",
        "    entity_dic = [x for x,y in entity_dic]\n",
        "    with open('entity_dic.json', 'w') as writer:\n",
        "        writer.write(json.dumps(entity_dic))\n",
        "\n",
        "    with open('classification_restrict_dic.json', 'w') as writer:\n",
        "      writer.write(json.dumps(classification_restrict_dic))\n",
        "\n",
        "    random.shuffle(output)\n",
        "    with open(output_file, 'w') as writer:\n",
        "        writer.write('\\n'.join(output))\n",
        "\n",
        "\n",
        "input_files = [\n",
        "    'nlu-benchmark/2017-06-custom-intent-engines/SearchCreativeWork/train_SearchCreativeWork_full.json',\n",
        "    'nlu-benchmark/2017-06-custom-intent-engines/RateBook/train_RateBook_full.json',\n",
        "    'nlu-benchmark/2017-06-custom-intent-engines/SearchScreeningEvent/train_SearchScreeningEvent_full.json',\n",
        "    'nlu-benchmark/2017-06-custom-intent-engines/AddToPlaylist/train_AddToPlaylist_full.json',\n",
        "    'nlu-benchmark/2017-06-custom-intent-engines/PlayMusic/train_PlayMusic_full.json',\n",
        "    'nlu-benchmark/2017-06-custom-intent-engines/GetWeather/train_GetWeather_full.json',\n",
        "    'nlu-benchmark/2017-06-custom-intent-engines/BookRestaurant/train_BookRestaurant_full.json'\n",
        "]\n",
        "classification_restrict_dic = {}\n",
        "entity_dic = {}\n",
        "parse_files(input_files, 'train_slots_dataset.json', \n",
        "            classification_restrict_dic, entity_dic)\n",
        "\n",
        "input_files = [\n",
        "    'nlu-benchmark/2017-06-custom-intent-engines/SearchCreativeWork/validate_SearchCreativeWork.json',\n",
        "    'nlu-benchmark/2017-06-custom-intent-engines/RateBook/validate_RateBook.json',\n",
        "    'nlu-benchmark/2017-06-custom-intent-engines/SearchScreeningEvent/validate_SearchScreeningEvent.json',\n",
        "    'nlu-benchmark/2017-06-custom-intent-engines/AddToPlaylist/validate_AddToPlaylist.json',\n",
        "    'nlu-benchmark/2017-06-custom-intent-engines/PlayMusic/validate_PlayMusic.json',\n",
        "    'nlu-benchmark/2017-06-custom-intent-engines/GetWeather/validate_GetWeather.json',\n",
        "    'nlu-benchmark/2017-06-custom-intent-engines/BookRestaurant/validate_BookRestaurant.json'\n",
        "]\n",
        "parse_files(input_files, 'validate_slots_dataset.json',\n",
        "            classification_restrict_dic, entity_dic)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJ8XgPXOm5o_",
        "outputId": "64ab8220-4f78-4582-b68e-7750daf0345d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Load dataset\n",
        "\n",
        "with open('entity_dic.json') as reader:\n",
        "  slot_labels_dict = json.load(reader)\n",
        "print('Dictionary:')\n",
        "for i, item in enumerate(slot_labels_dict):\n",
        "  print('    {}: \"{}\"'.format(i, item))\n",
        "\n",
        "with open('classification_restrict_dic.json') as reader:\n",
        "  classification_restrict_dic = json.load(reader)\n",
        "print('Classification restrict dictionary:')\n",
        "for k, v in classification_restrict_dic.items():\n",
        "  print('    {}: {}'.format(k, v))\n",
        "\n",
        "def create_dataset(input_file):\n",
        "  input_ids = []\n",
        "  attention_mask = []\n",
        "  labels = []\n",
        "  with open('train_slots_dataset.json') as reader:\n",
        "    for line in reader:\n",
        "      to_add = json.loads(line)\n",
        "      input_ids.append(to_add['input_ids'])\n",
        "      attention_mask.append(to_add['attention_mask'])\n",
        "      labels.append(to_add['labels'])\n",
        "    print('Sample from {}:'.format(input_file))\n",
        "    print('    Input ids:', input_ids[0][0:10])\n",
        "    print('    Attention mask:', attention_mask[0][0:10])\n",
        "    print('    Labels:', labels[0][0:10])\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {'input_ids':input_ids, 'attention_mask':attention_mask},\n",
        "    labels\n",
        "  ))\n",
        "  return dataset\n",
        "\n",
        "train_slots_dataset = create_dataset('train_slots_dataset.json')\n",
        "validate_slots_dataset = create_dataset('validate_slots_dataset.json')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dictionary:\n",
            "    0: \"\"\n",
            "    1: \"object_type\"\n",
            "    2: \"object_name\"\n",
            "    3: \"rating_value\"\n",
            "    4: \"best_rating\"\n",
            "    5: \"rating_unit\"\n",
            "    6: \"object_select\"\n",
            "    7: \"object_part_of_series_type\"\n",
            "    8: \"location_name\"\n",
            "    9: \"movie_name\"\n",
            "    10: \"object_location_type\"\n",
            "    11: \"timeRange\"\n",
            "    12: \"movie_type\"\n",
            "    13: \"spatial_relation\"\n",
            "    14: \"music_item\"\n",
            "    15: \"playlist\"\n",
            "    16: \"artist\"\n",
            "    17: \"playlist_owner\"\n",
            "    18: \"entity_name\"\n",
            "    19: \"track\"\n",
            "    20: \"service\"\n",
            "    21: \"year\"\n",
            "    22: \"album\"\n",
            "    23: \"sort\"\n",
            "    24: \"genre\"\n",
            "    25: \"geographic_poi\"\n",
            "    26: \"condition_description\"\n",
            "    27: \"current_location\"\n",
            "    28: \"condition_temperature\"\n",
            "    29: \"state\"\n",
            "    30: \"city\"\n",
            "    31: \"country\"\n",
            "    32: \"restaurant_name\"\n",
            "    33: \"restaurant_type\"\n",
            "    34: \"poi\"\n",
            "    35: \"served_dish\"\n",
            "    36: \"party_size_number\"\n",
            "    37: \"cuisine\"\n",
            "    38: \"facility\"\n",
            "    39: \"party_size_description\"\n",
            "Classification restrict dictionary:\n",
            "    SearchCreativeWork: [0, 1, 2]\n",
            "    RateBook: [0, 2, 3, 4, 5, 6, 1, 7]\n",
            "    SearchScreeningEvent: [0, 1, 8, 9, 10, 11, 12, 13]\n",
            "    AddToPlaylist: [0, 14, 15, 16, 17, 18]\n",
            "    PlayMusic: [0, 14, 19, 16, 20, 21, 22, 23, 15, 24]\n",
            "    GetWeather: [0, 11, 25, 26, 13, 27, 28, 29, 30, 31]\n",
            "    BookRestaurant: [0, 32, 33, 29, 11, 13, 34, 35, 36, 31, 30, 23, 37, 38, 39]\n",
            "Sample from train_slots_dataset.json:\n",
            "    Input ids: [101, 2338, 2033, 1037, 4825, 1999, 16283, 26822, 9777, 2008]\n",
            "    Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "    Labels: [0, 0, 0, 33, 0, 30, 30, 30, 0, 0]\n",
            "Sample from validate_slots_dataset.json:\n",
            "    Input ids: [101, 2338, 2033, 1037, 4825, 1999, 16283, 26822, 9777, 2008]\n",
            "    Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "    Labels: [0, 0, 0, 33, 0, 30, 30, 30, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhzzRGW2u2tG",
        "outputId": "7cbd0d7a-9336-4514-c1b8-7e0906d74eb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "# Train the model\n",
        "\n",
        "model = TFDistilBertForTokenClassification.from_pretrained('distilbert-base-uncased',\n",
        "                                                           num_labels=len(slot_labels_dict))\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy'])\n",
        "model.fit(train_slots_dataset.shuffle(100).batch(16),\n",
        "          epochs=3,\n",
        "          batch_size=16,\n",
        "          validation_data=validate_slots_dataset.shuffle(100).batch(16))\n",
        "model.save_pretrained('slots.model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py:493: UserWarning: Using `-1` to mask the loss for the token is deprecated. Please use `-100` instead.\n",
            "  return py_builtins.overload_of(f)(*args)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 69/108 [==================>...........] - ETA: 20s - loss: 0.7890 - accuracy: 0.8581"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nK3v4NqyRNH"
      },
      "source": [
        "# Load model and make prediction on specific sentence \n",
        "\n",
        "slots_model = TFDistilBertForTokenClassification.from_pretrained('slots.model')\n",
        "\n",
        "def fill_slots(test_sentence, restrict_labels=None):\n",
        "  def argmax(input):\n",
        "    if not restrict_labels:\n",
        "      return np.argmax(input)\n",
        "    max_value = -1e+10\n",
        "    max_index = -1\n",
        "    for i in restrict_labels:\n",
        "      val = input[i]\n",
        "      if val > max_value:\n",
        "        max_index = i\n",
        "        max_value = val\n",
        "    return max_index\n",
        "\n",
        "  tokenized = tokenizer.tokenize(test_sentence)\n",
        "  predict_input = tokenizer.encode(test_sentence,\n",
        "                                   return_tensors='tf')\n",
        "  tf_output = slots_model.predict(predict_input)[0][0, :, :]\n",
        "  length = min(tf_output.shape[0], len(tokenized))\n",
        "  predictions = [argmax(tf_output[_, :]) for _ in range(length)]\n",
        "\n",
        "  slots = []\n",
        "  for token, label in zip(tokenized, predictions):\n",
        "    if (slots and label == slots[-1][1]):\n",
        "      slots[-1][0] += token[2:] if token.startswith('##') else ' ' + token\n",
        "    else:\n",
        "      slots.append([token, label])\n",
        "\n",
        "  result = ''\n",
        "  for token, label in slots:\n",
        "    if label == 0:\n",
        "      result += token + ' '\n",
        "    else:\n",
        "      result += '[' + slot_labels_dict[label] + ': ' + token + '] '\n",
        "  return result.strip()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LnIME959xVD",
        "outputId": "2748882c-ac4f-4395-b6e3-796d23426ea4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Slot filling test\n",
        "\n",
        "print(fill_slots('add another song to cita romantica playlist'))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "add another [music_item: song] to [playlist: cita romantica] playlist\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZdARcfCmeAw"
      },
      "source": [
        "# Step 3: Complete test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8hQFP7QmZ2Q",
        "outputId": "a6e70155-48f0-4253-abef-502561d6c22e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "# Final test\n",
        "\n",
        "def predict_class_and_slots(test_sentence):\n",
        "  intent_class, probabilities = get_intent_class(test_sentence)\n",
        "  restrict_labels = classification_restrict_dic[intent_class]\n",
        "  slots = fill_slots(test_sentence, restrict_labels)\n",
        "  return intent_class, slots\n",
        "\n",
        "for test_sentence in ['play a jack lawrence concerto', \n",
        "                      'what is the weather today?',\n",
        "                      'i would like to add visjoner to my playlist']:\n",
        "  intent_class, slots = predict_class_and_slots(test_sentence)\n",
        "  print('Test sentence:', test_sentence)\n",
        "  print('Intent class:', intent_class)\n",
        "  print('Slots:', slots)\n",
        "  print()\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test sentence: play a jack lawrence concerto\n",
            "Intent class: PlayMusic\n",
            "Slots: play a [artist: jack lawrence] [music_item: concerto]\n",
            "\n",
            "Test sentence: what is the weather today?\n",
            "Intent class: GetWeather\n",
            "Slots: what is the weather [timeRange: today] ?\n",
            "\n",
            "Test sentence: i would like to add visjoner to my playlist\n",
            "Intent class: AddToPlaylist\n",
            "Slots: i would like to add [entity_name: visjoner] to [playlist_owner: my] playlist\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}